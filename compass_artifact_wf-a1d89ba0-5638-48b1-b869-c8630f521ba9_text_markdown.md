# Implementation Science Journal: Critical Pre-Submission Assessment for LAI-PrEP CDS Validation Study

**Your manuscript as currently framed—progressive validation of a clinical decision support tool using synthetic patients—is highly likely to be desk-rejected from Implementation Science journal.** The journal's 2024 editorial guidelines explicitly exclude pure tool validation studies, requiring instead that submissions focus on implementing already-validated, evidence-based interventions in real-world settings. Approximately 50% of submissions are rejected without peer review, primarily due to scope violations like this. However, with significant reframing and additional real-world implementation data, the work could potentially fit the journal's scope.

The journal's October 2024 editorial clarifies that "studies where the primary focus is on establishing the effectiveness of clinical, health service, or population health interventions" are routinely desk-rejected. Your validation study—even with progressive scaling—fundamentally asks "does this tool work?" rather than the implementation science question: "how do we help clinicians adopt this validated tool?" This distinction is non-negotiable for Implementation Science, which published this stark warning: **"the use of an established framework that derives from implementation research does not equate to the study of implementation."**

## Current journal landscape and your manuscript's fit

Implementation Science (Impact Factor: 13.4, 2024) ranks 4th of 174 journals in Health Care Sciences & Services and rejects roughly 60-70% of submissions. The journal underwent major scope refinement in October 2024, tightening requirements around what constitutes implementation research versus effectiveness research, tool development, or health informatics. Median time to first decision is 29 days, with desk rejections often occurring within 2 weeks.

**Critical scope boundaries for your work:** The journal distinguishes clinical decision support tool validation (out of scope) from CDS implementation strategies (in scope). A 2025 systematic review published in the journal examined CDSS implementation barriers using the Theoretical Domains Framework—but this studied barriers to implementing already-established tools, not validating new ones. The journal accepts validation studies only when they validate implementation constructs (e.g., new measures of acceptability, adoption, or fidelity), not clinical tools themselves.

**Synthetic validation acceptability:** No published examples exist of synthetic-only validation studies in Implementation Science. The literature review reveals that synthetic patient data, while useful for proof-of-concept and privacy-preserving research, cannot address implementation questions. Synthetic data systematically under-represents care deviations, organizational context, workflow integration challenges, and human factors—precisely the elements implementation science studies. One validation study of the Synthea synthetic data generator concluded that "synthetic data generators do not currently model for deviations in care," making them fundamentally incompatible with implementation research's real-world focus.

## Implementation Science journal requirements (2024-2025 standards)

### Scope and manuscript types

The journal's **core mission** states it "publishes research relevant to the scientific study of methods to promote the uptake of research findings into routine healthcare in clinical, organizational, or policy contexts." The 2024 editorial defines the central focus as "rigorous empirical studies assessing the effects of deliberate and purposive actions to implement evidence-based interventions, practices and policies."

**What the journal publishes:** Implementation strategy effectiveness studies (especially comparative trials), process evaluations of implementation contexts and mechanisms, economic evaluations of implementation strategies, rigorous large-scale intervention development, methodological papers with theoretical basis, de-implementation studies, and validation of measures of implementation constructs.

**What triggers desk rejection:** Studies establishing clinical/intervention effectiveness rather than implementation effectiveness, intervention studies lacking scientific rigor or hypothesis-driven design, descriptive accounts without analytic content or linkage to implementation literature, studies reporting only self-report outcomes, and highly descriptive determinants papers without theory or empirical gaps.

### Structural and formatting requirements

**Word limits are strictly enforced:** 5,500 words maximum for research articles, systematic reviews, and study protocols (main text only, excluding abstract/tables/figures). The 2024 editorial warns that "manuscripts exceeding the journal word limits for the respective article type may be returned without review."

**Abstract requirements:** Maximum 350 words with mandatory structured sections (Background, Methods, Results, Conclusions). For trials, CONSORT extension for abstracts is required, including trial registration number and date.

**Unique "Contributions to the Literature" requirement:** All manuscripts must include 3-5 bullet points (100 words total maximum) describing what the paper adds to implementation science literature. This **cannot simply restate findings**—it must contextualize the paper's contribution to the full implementation science literature. The journal uses this for assessing publication priority. Example from successful papers: "This is the first study to systematically map CDSS implementation barriers across all TDF domains and translate these to specific implementation strategies using the Behaviour Change Wheel."

**Mandatory reporting checklists:** Every manuscript requires populated checklists from EQUATOR Network guidelines. For implementation studies, StaRI is required. For intervention description, TIDieR or STaRI. For study design (CONSORT for RCTs, STROBE for observational, PRISMA for reviews). Missing or incomplete checklists result in manuscript return without review.

### Critical 2024 framework and theory requirements

The October 2024 editorial implements a **major policy shift** on framework application, moving from accepting surface-level framework use to requiring deep theoretical engagement. The journal states it is "increasingly reluctant to publish studies that categorize data according to a framework without offering interpretations that relate to the underlying theory."

**New standard:** Frameworks must explicitly inform research aims, guide data collection and analysis, shape findings presentation, and provide basis for articulating contributions. The editorial emphasizes "this shift from theories as 'products' to theorizing as a 'process' will contribute to advancement of knowledge."

**Most commonly used frameworks in successful publications:** CFIR (Consolidated Framework for Implementation Research—updated 2022 with 48 constructs), TDF (Theoretical Domains Framework—14 domains, often combined with Behaviour Change Wheel), RE-AIM (evaluation framework), ERIC taxonomy (73 implementation strategies), i-PARIHS, and Proctor's Implementation Outcomes Framework (8 distinct outcomes: acceptability, adoption, appropriateness, feasibility, fidelity, penetration, implementation cost, sustainability).

**Framework application pitfalls that lead to rejection:** Superficial or tokenistic application, mechanically categorizing data without theoretical interpretation, developing yet another framework without demonstrating necessity, applying frameworks not throughout the manuscript but only in discussion, and failing to justify framework selection over alternatives.

## Recent publications reveal what succeeds in this journal

### Clinical decision support systems in Implementation Science

The **2025 systematic review on CDSS implementation** (Implementation Science 20:33) exemplifies what the journal publishes. Key features: 99 studies examining barriers/facilitators to implementing already-developed CDSS tools, used TDF + Behaviour Change Wheel frameworks deductively and inductively, identified 2,563 unique barriers across all 14 TDF domains, and provided stakeholder-specific implementation recommendations for developers, primary care teams, and policymakers.

**Critical distinction:** This paper studied implementation of existing tools, not validation of new tools. The tools' clinical effectiveness was assumed/established; the research question focused on how to overcome implementation barriers. The paper addressed Environmental Context & Resources (time constraints, workflow integration, technical issues), Memory/Attention/Decision Processes (cognitive load, confirmation bias), and Beliefs about Capabilities (provider skepticism, trust issues with AI).

**Article structure pattern:** Background establishing implementation challenges → Methods using systematic review with framework-guided analysis → Results organized by framework domains → Discussion with practical, actionable implementation strategies → Conclusions emphasizing implementation insights (not tool effectiveness).

### Validation studies that succeed

Implementation Science accepts validation only for **implementation constructs**, not clinical tools. Example: A 2023 validation study of evidence-based practice instruments (Implementation Science 18:42) used COSMIN methodology to assess content validity, structural validity, internal consistency, and reliability of measures used in implementation research. The study validated instruments measuring implementation-relevant constructs (attitudes toward EBP, EBP behavior, self-efficacy) across healthcare disciplines—not clinical outcomes.

Another example: The updated CFIR framework (2022, Implementation Science 17:75) used mixed methods (systematic literature review + user survey of 334 authors) to refine and validate the framework itself based on 10+ years of user feedback. This meta-level validation of an implementation science tool is in scope.

**Psychometric standards when relevant:** Studies developing implementation outcome measures (e.g., Acceptability of Intervention Measure, Intervention Appropriateness Measure, Feasibility of Intervention Measure—Implementation Science 12:108) used multi-phase development with content validity assessment, exploratory and confirmatory factor analysis, strong internal consistency (α > 0.85), and known-groups validity. These 4-item measures assess implementation outcomes, not clinical effectiveness.

### HIV prevention and PrEP implementation research

**Limited direct HIV/PrEP publications in Implementation Science journal itself** during 2023-2025. The broader PrEP implementation science literature (published in journals like AIDS and Behavior) reveals successful approaches: Using i-PARIHS framework to guide barrier identification, mapping barriers to specific ERIC implementation strategies, focusing on implementation outcomes (PrEP awareness, initiation, uptake, adherence, retention), addressing equity explicitly (e.g., Black women, people who use drugs, transgender populations), and examining implementation strategies like normalizing PrEP in communications, provider education, and structural barrier reduction.

A successful pattern: Studies first establish that PrEP is evidence-based (cite systematic reviews, CDC guidelines), then study implementation strategies to increase uptake among specific populations facing barriers. The research question focuses on implementation (how to increase adoption), not effectiveness (does PrEP work).

## Common reviewer concerns and rejection patterns

### Primary rejection categories accounting for 60-70% of rejections

**Scope violations (approximately 50% of rejections):** The fundamental question is whether the study examines implementation of evidence-based interventions or establishes intervention effectiveness. Studies testing novel interventions where effectiveness is uncertain are rejected. Type 1 hybrid designs (primarily testing clinical effects while observing implementation) are borderline. Type 2 and 3 hybrids (major implementation component) are acceptable.

**Evidence base requirements:** Interventions must be supported by consolidated evidence—preferably systematic reviews or clinical guidelines. Single-study evidence is "borderline in scope." Your manuscript would need to cite systematic reviews demonstrating that CDS tools for LAI-PrEP bridge navigation are evidence-based practices worthy of implementation.

**Insufficient generalizability/external validity:** Single-site case studies with limited transferability are routinely rejected. The journal states: "As well as being interested in internal validity, we are very interested in studies that produce clearly generalizable results (have high external validity), or that provide evidence of transferability across settings." Multiple sites, multiple settings, or strong theoretical frameworks that enhance transferability are essential. Synthetic patient studies face inherent generalizability challenges since they cannot capture real-world organizational context, workflow variations, or implementation barriers.

**Poor integration with existing literature:** Manuscripts "not sufficiently embedded in the contemporary literature" or that "fail to build upon existing bodies of knowledge" are rejected. The journal expects international literature scope (not single-jurisdiction focus) and requires discussion sections that interpret findings in light of relevant literature rather than merely describing results. Many submissions are "sent back to authors or deemed to be of insufficient priority for review" due to inadequate literature contextualization.

**Methodological rigor failures:** Uncontrolled before-after designs, pilot studies without clear feasibility focus, single-site studies without theoretical justification, self-report outcomes only (without validated observable measures), and poor quality qualitative research (no saturation, lack of theory) trigger rejection. The journal requires rigorous experimental or quasi-experimental designs: cluster RCTs, interrupted time-series, difference-in-differences, or rigorous non-randomized controlled studies with clear justification.

### Specific validation study concerns

Implementation Science explicitly states it publishes validation only for "new measures of implementation constructs (unless many similar measures already exist)" and comprehensive validation reports (not incremental ones). Clinical tool validation—even with progressive scaling from 1K to 21.2M synthetic patients—does not meet this criterion because it validates the clinical tool's accuracy, not implementation constructs.

**Real-world data expectations:** The journal's focus on "routine healthcare in clinical, organizational, or policy contexts" requires real-world implementation evidence. Synthetic data cannot demonstrate observable aspects of healthcare delivery, practice changes, or professional behaviors—the outcomes the journal prioritizes. One editorial states the journal's interest in "observable aspects of healthcare delivery, changes in practice, and/or professional behaviours" with validated measures, explicitly discouraging studies with only self-report or perception measures.

## Framework requirements and how validation studies fit

### Implementation science versus clinical validation versus health informatics

The literature reveals a **critical conceptual distinction** that determines journal fit:

**Clinical/Tool Validation (OUT of scope):** Research question asks "Does the tool work?" Measures clinical outcomes, accuracy, discrimination, calibration. Focuses on internal validity in controlled conditions. Progressive validation across synthetic patients falls here—it's establishing tool effectiveness, not studying implementation.

**Implementation Research (IN scope):** Research question asks "How do we help people/places use the tool?" Measures implementation outcomes (adoption, fidelity, penetration, sustainability). Focuses on external validity in naturalistic settings. Requires the tool to already be validated/evidence-based, then studies strategies to promote uptake.

**Health Services Research (DIFFERENT field):** Examines quality, access, cost of care and practice variations, but may lack implementation science frameworks. Broader policy questions without specific implementation focus.

**Health Informatics (DIFFERENT field):** Focuses on design, development, and technical validation of health IT. Emphasizes algorithm accuracy, system integration, usability, and functionality—not organizational adoption strategies.

The 2021 Implementation Science editorial makes this explicit: "We note that the use of an established framework that derives from implementation research (for instance, the use of CFIR in a process evaluation that is part of a clinical trial) does not equate to the study of implementation. Frameworks are increasingly used, particularly in the evaluation of novel interventions, but such use does not necessarily imply a contribution to the knowledge in implementation science."

### When CDS tools belong in implementation science

**CDS papers that succeed:** Studies examining implementation strategies for adopting validated CDS tools (e.g., using PRISM framework + CDS best practices to design implementation approaches), barrier and facilitator analyses using TDF or CFIR for CDS adoption in hospitals, implementation outcome evaluations (adoption rates, fidelity to intended use, sustainability over time), and process evaluations explaining why CDS implementation succeeded or failed in different contexts.

**CDS papers that are rejected:** Technical development of CDS algorithms, clinical validation of CDS accuracy/effectiveness, usability testing without implementation focus, and CDS design studies without implementation strategies.

### Progressive validation and implementation science boundaries

**Your study's progressive validation approach (1K → 21.2M synthetic patients) represents:**
- Clinical/effectiveness research establishing the tool works
- Health informatics technical validation of simulated cases  
- Medical education research if used for training purposes

**NOT implementation science unless:**
1. The tool is positioned as already evidence-based (prior validation cited)
2. Study tests implementation strategies to increase clinician uptake
3. Measures implementation outcomes (not educational or clinical outcomes only)
4. Addresses barriers and facilitators using implementation frameworks
5. Evaluates implementation processes in real-world healthcare settings

The research continuum progresses: **Efficacy research** (ideal conditions, proves tool CAN work) → **Effectiveness research** (pragmatic conditions, proves tool DOES work) → **Implementation research** (natural conditions, proves tool can be ADOPTED and SUSTAINED). Your manuscript currently occupies the efficacy/effectiveness space, not the implementation space.

## Reporting standards for clinical decision support validation

### Multiple guidelines apply depending on research stage

**TRIPOD+AI (2024):** Primary guideline for predictive models underlying CDS tools. Covers 27 items including data sources, participant characteristics, model development/validation methods, discrimination metrics (c-statistic, AUC), calibration assessment, clinical utility evaluation, and code/data sharing. This guideline applies to your technical validation work but focuses on model accuracy, not implementation.

**DECIDE-AI (2022):** For early-stage clinical evaluation of AI-based decision support systems. Focuses on human-in-the-loop decision-making and small-scale implementation. Requires evaluation during actual clinical use (not just technical validation), human factors assessment, workflow integration, and safety evaluation. This bridges technical validation and implementation but emphasizes small-scale, exploratory evaluation.

**StaRI (2017):** Standards for Reporting Implementation Studies. Required for Implementation Science journal submissions. Contains 27 items covering dual strands: (1) the implementation strategy and (2) the intervention being implemented. Key requirements include context description, implementation strategy details, barriers and facilitators, implementation outcomes (acceptability, adoption, appropriateness, feasibility, fidelity, penetration, sustainability), process evaluation, and clinical outcomes (secondary to implementation outcomes).

**Critical finding:** Your study would need TRIPOD+AI for the validation component, but Implementation Science requires StaRI. The disconnect reveals the scope mismatch—StaRI cannot be completed for pure validation studies lacking implementation strategies and real-world implementation data.

### Synthetic validation acceptability verdict

**Synthetic validation alone is NOT acceptable for implementation science journals.** The evidence reveals:

**Technical and methodological acceptability:** Synthetic data like Synthea can achieve fidelity (70-85% accuracy in replicating statistical properties). Validation studies show synthetic data useful for tool development, privacy-preserving research, hypothesis generation, and proof-of-concept work.

**Implementation science incompatibility:** Synthetic data systematically under-represents care deviations, organizational context, workflow integration challenges, and human factors. A 2019 validation study concluded that synthetic data shows "perfect/near-perfect scores on quality measures (unrealistic)" and "do not currently model for deviations in care"—precisely what implementation science studies. Implementation research requires barriers that emerge from real organizational contexts, staffing constraints, competing priorities, and human behavior variations that synthetic data cannot capture.

**Published precedent:** Searches revealed no examples of synthetic-only validation studies published in Implementation Science. High-quality journals publishing synthetic data research (JCO Clinical Cancer Informatics, Nature Digital Medicine, BMC Medical Informatics) explicitly frame it as methodology development, not implementation research, and consistently state synthetic validation cannot substitute for real-world studies.

**FDA and regulatory perspective:** Clinical Decision Support Software guidance (2022) requires demonstration of safety and effectiveness. Synthetic data alone is insufficient for regulatory approval. The AISAMD program is developing standards but they remain incomplete.

### No established standards exist for synthetic-to-real-world generalization

A critical gap in the literature: **No published guidelines specifically address how to generalize from synthetic patient validation to real-world implementation.** A 2020 BMC Medical Research Methodology study states: "There is no benchmark or standard for validating synthetic data."

**Proposed bridging evidence** (synthesized from literature, not established standards): Demonstrate synthetic data limitations explicitly, conduct validation comparing synthetic versus real data performance, document systematic differences, justify generalization with evidence, then complete external validation in multiple real-world datasets across diverse populations and settings.

**The burden of proof rests entirely on researchers** to demonstrate why synthetic findings would generalize—and Implementation Science journal's emphasis on external validity and real-world settings makes this burden exceptionally high.

## Actionable recommendations for your manuscript

### Critical decision point: Is Implementation Science the right venue?

**Your manuscript likely fits better in:**
- **Medical Education journals** (Academic Medicine, Medical Education, Teaching and Learning in Medicine) if framing emphasizes educational outcomes and progressive validation methodology
- **Simulation journals** (Simulation in Healthcare) for synthetic patient methodology and progressive validation approach  
- **Health Informatics journals** (Journal of the American Medical Informatics Association, Journal of Medical Internet Research) for CDS tool development and validation
- **HIV/Prevention journals** (AIDS and Behavior, Journal of Acquired Immune Deficiency Syndromes, Journal of the International AIDS Society) if emphasizing clinical application to PrEP bridge navigation
- **Implementation Science Communications** (companion journal with broader scope) possibly, but still requires real-world implementation component

### If you pursue Implementation Science journal submission, major reframing required

**Essential elements to add:**
1. **Establish LAI-PrEP bridge navigation CDS as evidence-based:** Cite systematic reviews or clinical guidelines supporting CDS for PrEP navigation (if these exist). If consolidated evidence doesn't exist, your tool cannot yet be "implemented" in the journal's framework.

2. **Shift research question from validation to implementation:** Primary question must become "What implementation strategies facilitate adoption of LAI-PrEP CDS by providers?" not "How accurate is the CDS across scales?"

3. **Add real-world implementation phase:** Collect data from actual healthcare settings implementing the tool. Measure observable practice changes, not just synthetic patient outcomes.

4. **Measure implementation outcomes explicitly:** Using Proctor's framework, assess acceptability (provider perceptions), adoption (decision to use tool), appropriateness (fit for setting), feasibility (can it be carried out), fidelity (used as intended), penetration (integration into services), implementation cost, and sustainability.

5. **Apply implementation framework deeply:** Select CFIR, TDF, RE-AIM, or i-PARIHS. Use it to guide research aims, data collection, analysis, findings presentation, and discussion. Go beyond categorizing data—interpret findings through theoretical lens.

6. **Conduct barrier and facilitator analysis:** Use framework domains (e.g., TDF's 14 domains including Knowledge, Skills, Environmental Context, Social Influences) to identify and categorize implementation barriers and facilitators from real-world pilot sites.

7. **Describe implementation strategies:** What specific actions promote CDS adoption? Use ERIC taxonomy to name and describe strategies (e.g., educational meetings, audit and feedback, clinical champions, EHR integration support).

8. **Complete StaRI reporting checklist:** This dual-strand framework requires describing both the implementation strategy and the clinical intervention. Your current validation focus addresses only the intervention strand.

### Manuscript structure recommendations

**If reframed for Implementation Science:**

**Background section (current gap):** Position LAI-PrEP bridge period as established evidence-based practice requiring clinical decision support. Cite PrEP effectiveness evidence, bridge navigation importance, and gap between evidence and practice. Frame the problem as implementation failure, not knowledge gap. State: "Despite evidence supporting LAI-PrEP bridge navigation, uptake remains low due to implementation barriers."

**Methods section (substantial additions needed):** Describe implementation strategy (not just tool development). Name the implementation framework guiding the work. Describe real-world pilot sites, organizational contexts, and participants. Specify implementation outcomes measured. Detail data collection for barriers/facilitators. The synthetic validation component becomes background/tool description, not the primary method.

**Results section (major refocus):** Lead with implementation outcomes (adoption rates, fidelity measures, provider acceptability ratings). Present barriers and facilitators organized by framework domains. Show how barriers vary across contexts. Tool accuracy becomes secondary or is assumed/cited from prior work. Emphasize observable practice changes and implementation processes.

**Discussion section (complete reorientation):** Interpret findings through implementation science lens. Discuss implications for implementation strategies in diverse settings. Connect to implementation science literature (not just CDS or PrEP literature). Address how findings advance understanding of CDS implementation challenges. Discuss transferability to other CDS tools or PrEP settings.

**Required additions:**
- "Contributions to the Literature" section (3-5 bullets, 100 words max) stating what this adds to implementation science specifically
- Data availability statement
- Complete StaRI checklist as supplementary file
- Ethics approval documentation
- If no real-world data: cannot complete these requirements

### Alternative framing: Methodological innovation paper

**Potential angle if staying close to current work:** Frame as methodological paper on progressive validation approach for CDS implementation readiness. This is highly challenging and would require:

**Theoretical contribution:** Argue that progressive validation across scales represents a novel method for assessing implementation readiness. Connect to implementation science by proposing validation metrics predict real-world adoption barriers.

**Empirical demonstration:** Show that findings from larger-scale synthetic validation predict implementation challenges in actual pilot sites. Demonstrate correspondence between synthetic findings and real-world barriers.

**Framework integration:** Use NASSS (Nonadoption, Abandonment, Scale-up, Spread, and Sustainability) or similar to analyze how progressive validation informs each implementation phase.

**Limitations acknowledgment:** Explicitly state this is early-stage methodology requiring validation through real-world implementation studies.

**Even this reframing would likely face skepticism** without substantial real-world implementation data demonstrating the method's utility.

### What not to do

**Common mistakes that guarantee rejection:**

1. **Submitting validation study with superficial framework mention:** Adding a CFIR or TDF reference in the discussion without using it throughout does not make it implementation science.

2. **Claiming synthetic data shows "implementation feasibility":** Synthetic validation demonstrates technical feasibility, not implementation feasibility. These are distinct constructs.

3. **Focusing on clinical outcomes:** Patient outcomes from using the tool (even simulated patients) are not implementation outcomes. The journal prioritizes provider adoption, organizational integration, and sustainability.

4. **Single-setting pilot without theory:** A small pilot in one clinic without theoretical framework and generalizability discussion will be rejected as insufficient external validity.

5. **Post-hoc framework application:** Categorizing your existing results into framework domains after data collection appears tokenistic. Framework must guide design prospectively.

6. **Overgeneralizing from synthetic data:** Claiming that 21.2 million synthetic patients demonstrate real-world implementation success is methodologically unsound.

7. **Missing evidence base:** If you cannot cite systematic reviews showing CDS for LAI-PrEP bridge navigation is evidence-based, the intervention isn't ready for implementation research.

## Implementation science frameworks: detailed application guidance

### CFIR application for your context

If using **Consolidated Framework for Implementation Research** (updated 2022), the approach would be:

**Define the innovation:** LAI-PrEP bridge period clinical decision support tool (must be evidence-based—cite prior validation or guidelines supporting this approach)

**Identify inner setting:** Healthcare organizations implementing LAI-PrEP services (e.g., sexual health clinics, community health centers, infectious disease clinics)

**Research question:** What organizational and provider-level determinants (CFIR constructs) predict successful adoption and sustained use of LAI-PrEP bridge CDS?

**CFIR domains to address:**
- **Innovation Characteristics:** Evidence strength, adaptability, trialability, complexity, design quality, cost
- **Outer Setting:** Patient needs, external policies (e.g., PrEP reimbursement), peer pressure from other clinics
- **Inner Setting:** Structural characteristics, networks/communications, culture, implementation climate, readiness for implementation
- **Individuals:** Knowledge/beliefs about CDS, self-efficacy, stage of change, personal attributes
- **Process:** Planning how to introduce CDS, engaging stakeholders, executing implementation, reflecting and evaluating

**Data collection:** Qualitative interviews with providers about CFIR domains, surveys measuring CFIR constructs, observation of CDS use in workflow, document analysis of organizational policies

**Analysis:** Code barriers and facilitators to each CFIR construct. Identify which constructs predict adoption and fidelity. Develop implementation strategies targeting key barriers.

**Deep engagement:** Don't just categorize findings into CFIR bins. Interpret how CFIR constructs interact. For example: "While the innovation had strong evidence strength and design quality (Innovation Characteristics), poor implementation climate in the Inner Setting—specifically lack of dedicated implementation leadership and incompatible workflows—undermined adoption. This finding extends CFIR by demonstrating that Inner Setting constructs can override positive Innovation Characteristics in under-resourced settings."

### RE-AIM application for your context

If using **RE-AIM** (Reach, Effectiveness, Adoption, Implementation, Maintenance):

**Reach:** What proportion of eligible providers/clinics adopt LAI-PrEP bridge CDS? How representative are adopters versus non-adopters? Characterize equity dimensions.

**Effectiveness:** Does CDS implementation improve bridge navigation outcomes? (This connects to your validation work but focuses on real-world implementation effects, not controlled validation.)

**Adoption:** What proportion of targeted settings implement the CDS? What organizational characteristics predict adoption? Is adoption equitable across clinic types?

**Implementation:** Fidelity—is CDS used as intended? Adaptation—how do sites modify the tool? Cost—what resources are required? Time—what does the implementation timeline look like?

**Maintenance:** Is CDS use sustained at 6, 12, 24 months post-implementation? What factors predict sustainability? How does effectiveness change over time?

**Your study would measure all five RE-AIM dimensions** across multiple implementation sites, not just validate the tool in synthetic patients.

### TDF + Behaviour Change Wheel application

If using **Theoretical Domains Framework** (14 domains):

**Phase 1—Barrier identification:** Interview providers implementing LAI-PrEP bridge CDS. Code barriers to TDF domains:
- **Knowledge:** Do providers understand bridge navigation protocols?
- **Skills:** Can they interpret CDS recommendations?
- **Memory, Attention, Decision Processes:** Does CDS reduce cognitive load or increase it?
- **Behavioral Regulation:** Can providers incorporate CDS into workflow?
- **Social/Professional Role:** Is CDS use seen as part of their role?
- **Beliefs about Capabilities:** Do providers trust the CDS accuracy?
- **Beliefs about Consequences:** Do they believe using CDS improves patient outcomes?
- **Motivation and Goals:** Is CDS use prioritized?
- **Environmental Context and Resources:** Do they have time, IT support, workflow integration?
- **Social Influences:** Do colleagues support CDS use?
- **Emotion:** Does CDS create anxiety or confidence?

**Phase 2—Strategy selection:** Use Behaviour Change Wheel to map TDF barriers to intervention functions, then to specific behavior change techniques:
- Example: Barriers in "Skills" domain → Intervention function: Training → BCT: Practice/rehearsal
- Example: Barriers in "Environmental Context" → Intervention function: Enablement → BCT: EHR integration, workflow redesign

**Phase 3—Evaluation:** Test whether implementation strategies addressing identified barriers improve CDS adoption and fidelity.

**The 2025 CDSS systematic review** in Implementation Science demonstrates this approach: 2,563 barriers mapped to TDF domains → stakeholder-specific recommendations using Behaviour Change Wheel.

## Timeline and strategic recommendations

### Immediate actions (within 2 weeks)

**Decision point 1:** Determine if you have access to real-world implementation data or can collect it. If yes, Implementation Science becomes possible with major reframing. If no, select alternative journal targets.

**Decision point 2:** Assess evidence base for LAI-PrEP bridge CDS. Search for systematic reviews, meta-analyses, or clinical guidelines supporting CDS for PrEP bridge navigation. If consolidated evidence exists, you can position your tool as implementing evidence-based practice. If not, the journal will consider your intervention not yet ready for implementation research.

**Decision point 3:** Evaluate your goals. If your goal is to disseminate validation methodology and establish tool accuracy, Implementation Science is the wrong venue. If your goal is to contribute to knowledge about implementing CDS in PrEP settings, substantial additional work is needed.

### Short-term strategic options (1-3 months)

**Option A—Pivot to appropriate journal:** Submit current manuscript (progressive validation focus) to JAMIA, Simulation in Healthcare, or AIDS and Behavior with minimal revisions. These journals value methodological innovation in CDS validation and synthetic patient approaches.

**Option B—Pilot implementation study:** Conduct small-scale implementation in 2-3 real-world PrEP clinics. Collect implementation outcomes and barrier/facilitator data. This creates hybrid Type 2 design (testing both effectiveness and implementation). Add this as new data to manuscript, reframe entirely, then target Implementation Science Communications (broader scope than flagship journal).

**Option C—Multi-phase paper strategy:** Publish current validation work in methods/informatics journal. Design and conduct implementation study as separate project. Publish implementation findings in Implementation Science, citing prior validation paper as establishing evidence base.

### Long-term considerations for implementation research trajectory

**Building an implementation science program requires:**

1. **Interdisciplinary team:** Include implementation scientists, not just clinicians and informaticians. Implementation Science strongly encourages diverse teams.

2. **Theory training:** Invest in understanding CFIR, TDF, RE-AIM deeply. Attend implementation science workshops (e.g., through SIRC—Society for Implementation Research Collaboration).

3. **Multi-site partnerships:** Develop relationships with diverse PrEP-providing organizations willing to participate in implementation research.

4. **Funding alignment:** Seek implementation science funding (NIH uses PAR-19-274 and similar for implementation research). Grants require clear distinction between effectiveness and implementation aims.

5. **Measurement infrastructure:** Identify or develop validated measures of implementation outcomes. Use resources like the Implementation Outcomes Repository and SIRC Instrument Review Project.

6. **Longitudinal perspective:** Implementation research requires sustained engagement. Plan for multi-year studies tracking adoption, fidelity, and sustainment.

## Final assessment and critical decision framework

### Journal fit assessment table

| Journal Option | Fit Score | Revision Magnitude | Timeline | Success Probability |
|----------------|-----------|-------------------|----------|-------------------|
| Implementation Science | Very Low | Complete rewrite + new data collection | 6-12+ months | <10% without real-world data |
| Implementation Science Communications | Low | Major reframe + pilot data | 3-6 months | 20-30% with small pilot |
| JAMIA (Journal of the American Medical Informatics Association) | High | Minor revisions | 1 month | 60-70% |
| Simulation in Healthcare | High | Minor revisions | 1 month | 60-70% |
| AIDS and Behavior | Moderate-High | Moderate revisions emphasizing PrEP context | 1-2 months | 50-60% |
| Academic Medicine | Moderate | Reframe for educational context | 2 months | 40-50% |

### When to target Implementation Science

**Submit to Implementation Science IF AND ONLY IF you can answer YES to ALL:**

1. ☐ Your intervention (LAI-PrEP bridge CDS) is supported by consolidated evidence (systematic reviews/guidelines) demonstrating its effectiveness
2. ☐ Your primary research question addresses implementation strategies, barriers, or processes—not tool validation or effectiveness
3. ☐ You have real-world implementation data from actual healthcare settings (not simulated)
4. ☐ You measure implementation outcomes (adoption, fidelity, acceptability, etc.) using validated instruments or rigorous qualitative methods
5. ☐ You apply implementation science framework(s) throughout the study (prospectively, not post-hoc)
6. ☐ Your study has multi-site data OR strong theoretical justification for single-site with explicit transferability discussion
7. ☐ You can articulate clear, substantial contribution to implementation science literature (not just CDS or PrEP literature)
8. ☐ You have completed or can complete StaRI checklist comprehensively
9. ☐ Your study design is rigorous (not uncontrolled before-after or purely descriptive)
10. ☐ You're prepared for extensive revisions and 4-6 month review process

**If you cannot check all boxes, Implementation Science will desk-reject your manuscript.** This is not a reflection of your work's quality—it's a scope mismatch. The progressive validation of synthetic patients represents valuable methodological work that deserves publication in an appropriate venue where it will be recognized and cited.

### Bottom-line recommendation

**Do not submit your current manuscript to Implementation Science.** The scope mismatch is fundamental, not superficial. Your work makes important contributions to CDS validation methodology, health informatics, and PrEP clinical care—but these are not contributions to implementation science as the journal defines it. Desk rejection is nearly certain, wasting 2-4 weeks and delaying publication in a more appropriate venue.

**Recommended path forward:** Submit to JAMIA or Simulation in Healthcare where progressive validation methodology will be valued. If you're passionate about contributing to implementation science, design a follow-up study that implements your validated tool in real-world settings and measures implementation outcomes. That subsequent study would be appropriate for Implementation Science.

The journal's October 2024 editorial concludes with guidance applicable to your situation: The journals receive manuscripts "that are clearly not within our scope or that fail to make a clear contribution to the field generally. We routinely desk reject these manuscripts (and offer transfer to other BMC journals)." Your manuscript isn't failing to contribute to research broadly—it's contributing to the wrong field for this specific journal. Choose a venue that will appreciate and amplify your methodological innovation in CDS validation rather than rejecting it for scope violations beyond your control.